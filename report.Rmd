---
title: Cloud-native Application Quality Model Validation Survey Report
author: Robin Lichtenth√§ler, Jonas Fritzsch
header-includes:
  - \usepackage{subfig}
urlcolor: blue
linkcolor: RoyalBlue
output: 
  bookdown::pdf_document2:  
    latex_engine: xelatex    # bookdown needs to be installed: install.packages("bookdown")
    keep_tex: false
  bookdown::html_document2:
always_allow_html: true
mainfont: DejaVu Sans # if not available, install it from https://dejavu-fonts.github.io/
sansfont: DejaVu Sans
monofont: DejaVu Sans Mono
---

\newpage

# Introduction{#intro}

This report analyzes the answers from a survey conducted through an online questionnaire in the period from October 2022 until December 2022. The aim of the survey was to empirically validate elements of a previously created [quality model for cloud-native application architectures](https://r0light.github.io/cna-quality-model/). More specifically, the quality model consists of so-called *factors* which represent architectural characteristics of software systems and so-called *quality aspects* which represent higher-level quality attributes mainly adopted from the [ISO 25010 standard](https://iso25000.com/index.php/en/iso-25000-standards/iso-25010).
Factors and quality aspects are connected through *impacts* which describe how the presence of a factor in a software architecture impacts the respective quality aspect. Through this, a hierarchical quality model can be created.
The survey focused on these impacts with the goals of:

1. Confirming the impacts stated in the initially formulated, literature-based quality model
2. Identifying additional impacts not considered yet in the quality model
3. Evaluating the strength of impacts

In the following, the setup of the survey is described in [section 2](#setup) and some general statistics on how the survey was answered by participants are reported in [section 3](#general) together with demographic information in [section 4](#demographics).
The main results of the survey regarding the validation of the quality model are presented in [section 5](#results) and a conclusion with a short discussion of these results is given in [section 6](#conclusion).

\newpage

# Survey setup{#setup}

## Survey Design and Interface

Because the quality model is intended to cover the breadth of the topic of cloud-native applications and considers multiple quality aspects in combination, it has a large number of factors, currently 76, which are arranged in an hierarchical graph with the quality aspects they impact at the top. To validate all these factors in a survey is challenging, therefore we:

1. considered only the impacts from factors on quality aspects, excluding impacts from factors on other factors within the hierarchy,
2. excluded factors merely serving as mediating factors between factors and quality aspects,
3. excluded factors which are less specific to cloud-native applications.

However, this still led to 45 factors for the survey together with 24 quality aspects which can be impacted. To enable participants to state any impact without being biased, all potential impacts, that means 45 * 24 = 1080, need to be considered, leading to the problem of how to design such a survey. 

To tackle this, we developed a [custom survey frontend](https://github.com/r0light/qmsurvey-frontend) which aims to make the rating of impacts from factors on quality aspects simple and intuitive. 
At the core, a participant is presented with a factor explained by a short description and the quality aspects grouped by their high-level quality aspects. 
By clicking on a quality aspect, it is possible to rate the impact from that factor on that quality aspect. 
Therefore, in each question a participant has to consider only one factor. 
Because the question page is structurally the same for each factor, the participant should get used to the format reasonably quickly.

In addition, we decided that participants could freely choose for how many factors they wanted to provide an answer. In an ideal case, each participant would provide an answer to each factor (45), but the time and effort one is willing to invest in such a survey is limited which limits the number of factors for which a participant is willing to provide an answer. Then again, there are differences between participants: some interested participants might be willing to provide more answers, others are not and might even abort the survey if the number of questions is too high. 
Thus, instead of asking for a random number of factors, we designed the survey flexible so that each participant could choose the factors to be answered based on their topics of interest and also their available time. Also, for each factor it was possible to choose any number of quality aspects that would be impacted by this factor.

## Survey distribution & participation

The intended target audience for the survey were IT professionals who have practical experience with implementing and deploying web applications on cloud infrastructures. This covers a rather broad range of professionals which is why we also distributed the survey in a broader approach. To ask for participation, we:

* contacted personally known professionals via email
* contacted professionals who had published articles on a fitting topic online
* contacted professionals who held a talk with a fitting topic at industry conferences
* contacted professionals who had published with a fitting topic at scientific conferences
* posted the request for participation on social media, including community groups specifically considering cloud computing

On the welcome page for the survey we stated the purpose of the survey and who would be eligible to participate so that interested persons could decide for themselves whether they wanted to fill out the survey. During the period in which the survey was available online, we received 42 complete submissions. The resulting data from these submissions as well as this report can be found online: 
[https://github.com/r0light/qmsurvey-results](https://github.com/r0light/qmsurvey-results)

```{r load-packages, include=FALSE}
library(extrafont)
library(tidyverse)
library(hrbrthemes)
library(viridis)
library(plyr)
library(dplyr)
library(knitr)
library(ggplot2)
library(hash)
library(gridExtra)
library(showtext)
library(ggpubr)
library(ggtext)
library(kableExtra)
library(XNomial)
showtext_auto()

```

```{r, include=FALSE, cache=TRUE}
extrafont::loadfonts(device="all", quiet = TRUE)

# load data (keep this as-is)
surveydata <- read.csv2("./data/surveyData.csv")
factorDetails <- read.csv2("./data/factors.csv")
aspectDetails <- read.csv2("./data/qualityaspects.csv")
initialImpacts <- read.csv2("./data/initialImpacts.csv")
demographics <- read.csv2("./data/demographics.csv")

# transform factor and quality aspect details

factorInformation <- hash()
apply(factorDetails, 1, function(factor) factorInformation[[factor["factorKey"]]] <- c(factor["factorName"], factor["factorDescription"]))

aspectInformation <- hash()
apply(aspectDetails, 1, function(aspect) aspectInformation[[aspect["qualityAspectKey"]]] <- c(aspect["qualityAspect"], aspect["qualityAspectDescription"], aspect["highLevelAspect"], aspect["highLevelAspectName"]))
```

\newpage

# General statistics{#general}

## Amount and distribution of answers

```{r boxplots, echo=FALSE, fig.width=3, fig.height=3.5, fig.show='hold', fig.align='center', fig.cap="No. of factors answered by participants and No. of ratings stated across all factors", cache=TRUE}

# number of answered factors per participant
numberOfAnsweredFactors <- ddply(surveydata, .(participant), function(df) c(count=nrow(df)))
numberOfAnsweredFactors %>%
  ggplot( aes(x="Across all participants", y=count)) +
  geom_boxplot(fill="steelblue", alpha = 0.6, width=0.4) +
  scale_y_continuous(breaks=sequence(c(20), from=0, by=2) ) + 
  theme_ipsum(base_family = "sans") +
  theme(
    legend.position="none",
    plot.title = element_text(size=11)
  ) +
  xlab("") +
  ylab("Factors answered")


# number of rated quality aspects per factor answer
reduced <- subset(surveydata, select=-c(factorKey, answerTime))
numberOfRatings <- reduced %>%
                   rowwise(participant) %>%
                   do((.) %>% as.data.frame %>% 
                        mutate(sum_rated = sum(.!=0) - 1))  %>% # use -1 because otherwise the "participant" column is included
                   select(participant, sum_rated)
numberOfRatings %>%
  ggplot(aes(x="Across all factors", y=sum_rated)) +
  geom_boxplot(fill="steelblue", alpha = 0.6, width=0.4) +
  scale_y_continuous(breaks=sequence(c(13), from=0, by=2) ) + 
  theme_ipsum(base_family = "sans") +
  theme(
    legend.position="none",
    plot.title = element_text(size=11)
  ) +
  xlab("") +
  ylab("Quality aspects rated per factor")


```

Because of the flexible survey design, it is interesting to analyze how participants made use of this flexibility, that means for how many factors each participant provided an answer and how many quality aspects they selected to rate for each factor.
This can be seen in Figure \@ref(fig:boxplots). In the left boxplot we can see that the median for how many factors were answered per participant is `r median(numberOfAnsweredFactors$count)` which is quite low compared to the number of 45 factors in total. It shows the difficulty of validating larger quality models using such an online questionnaire. 
Nevertheless, there were also participants who provided answers to significantly more factors which is helpful and would not have been possible with a non-flexible survey design in which the number of factors to answer was fixed beforehand.
The right boxplot shows for how many quality aspects ratings were stated across all factors. 
For a better understanding of this data it has to be noted that for each quality aspect participants could specify the impact based on the following scheme:

* the factor has a **positive impact (++)** on the quality aspect
* the factor has a **slightly positive impact (+)** on the quality aspect
* the factor has **no impact (0)** on the quality aspect
* the factor has a **slightly negative impact (-)** on the quality aspect
* the factor has a **negative impact (--)** on the quality aspect

The impact type *no impact* was selected as a default for each quality aspect, if no other impact was explicitly stated. 
The boxplot on the right of Figure \@ref(fig:boxplots) shows per factor, for how many of the 24 quality aspects a participant stated impacts explicitly. 
In the instructions of the survey we told participants that we would expect "*typically between one and five*" quality aspects for which impacts are stated.
As the median of `r median(numberOfRatings$sum_rated)` shows, most participants stayed within this range, but there are numerous outliers where participants stated impacts for many quality aspects. 
In one case, a participant stated impacts for all 24 quality aspects which might be the result of a misunderstanding of how to fill out the survey.

In addition, it is interesting to analyze which factors were selected by participants. Figure \@ref(fig:timesFactorsAnswered) shows for each factor, by how many participants it was selected to state impacts on quality aspects. While it is difficult to interpret why certain factors where chosen more often than others, it can be seen that there are significant differences between the factors. 

Potential reasons might be that certain factors are easier to understand, such as *Service replication* while others such as *Communication partner abstraction* are more abstract and therefore more difficult to evaluate.
The number of times answers have been provided for a factor will also be important for the interpretation of the results in [section 5](#results), because the results are only interpretable based on the set of participants who provided an answer which is nor representative for the whole set of participants.
Additionally, only a larger number of answers leads to more confidence in the interpretation of results.

```{r timesFactorsAnswered, echo=FALSE, fig.width=8, fig.height=8, fig.cap="Number of times the individual factors have been answered"}

answersPerFactor <- ddply(surveydata, .(factorKey), function(df) c(count=nrow(df)))
answersPerFactor$factorName <- as.character(lapply(answersPerFactor$factorKey, function(row) values(factorInformation, keys=row)[1]))
answersPerFactor <- answersPerFactor[order(-answersPerFactor$count),]
ggplot(answersPerFactor, aes(x=reorder(factorName, count), y=count)) + 
  geom_bar(stat = "identity", fill="steelblue", alpha = 0.9) +
  geom_text(aes(label = count), hjust = 2, color="white", size=4) +
  scale_y_continuous(breaks=sequence(c(max(answersPerFactor$count)), from=1, by=1) ) + 
  theme_ipsum(base_family = "sans") +
  theme(text = element_text(size=10), axis.text.x = element_text(size=10), axis.text.y = element_text(size=10)) +
  xlab("Factor") +
  ylab("Times answered") + 
  coord_flip()
```



## Answer times

```{r answerTimes, echo=FALSE, fig.width=5, fig.height=4, fig.cap="Time spent for answering individual factors"}

cleanedAnswerTimes <- surveydata %>%
                      select(answerTime) %>%
                      filter(answerTime > 0 & answerTime < 3600) %>%                      # remove erroneous data points
                      filter(!(abs(answerTime - median(answerTime)) > 2*sd(answerTime)))  # remove outliers
  
cleanedAnswerTimes %>%
  ggplot(aes(x="Across all Ratings", y=answerTime)) +
  geom_boxplot(fill="steelblue", alpha = 0.6, width=0.4) +
  #scale_y_continuous(breaks=sequence(c(13), from=0, by=2) ) + 
  theme_ipsum(base_family = "sans") +
  theme(
    legend.position="none",
    plot.title = element_text(size=11)
  ) +
  ggtitle("Answer times per factor in seconds") +
  xlab("") +
  ylab("Answer Time (s)")

```


Finally, the survey tool also captured a timestamp each time a factor was answered by a participant. 
This way, we can to some extent evaluate how much time participants spent on thinking about factors. 
However, an exact evaluation of such answer times would only be possible in a controlled experiment while with the online distribution of the survey we do not know how participants filled out the survey. For example, it was also possible to start the survey and continue at a different time, because the progress was stored locally in the participant's browser. 
Therefore, some answer times that were captured showed values of 188262 or 10861 seconds which are the result of such behaviour and therefore need to be excluded. 
As a countermeasure we firstly excluded obviously erroneous answer time values greater than 3600 seconds and based on the remaining data points, we secondly removed outliers which were greater than the doubled standard deviation. 
The remaining data is plotted in Figure \@ref(fig:answerTimes).
The median is at `r median(cleanedAnswerTimes$answerTime)` seconds, so slightly below one minute. 
The lower quartile is at `r quantile(cleanedAnswerTimes$answerTime)[2]` seconds and the upper quartile is at `r quantile(cleanedAnswerTimes$answerTime)[4]` seconds. 
The measured times seem reasonable, although there is also a significant amount of outliers which might be the result of factors that are difficult to answer. 
This data could be useful for planning future surveys with similar types of questions.

\newpage

# Demographics{#demographics}

In addition to the main part of the survey, we also asked some final demographics questions including the primary area to which participants would assign their current job to, their current job title, as well as the years of experience they have with *software development in general* on the one hand and *experience with cloud platforms* on the other hand.
It has to be noted that answering these questions was optional and only 34 participants reported demographic data at least partly.
Nevertheless, in Figure \@ref(fig:jobAreas) it can be seen that nearly half of the participants work in the industry area, in contrast to the other half stemming from academia, which provides a diversified field of participants.
And as Figure \@ref(fig:experience) shows, the participants also have significant experience. An expectable, but also interesting fact is that the experience with cloud computing is consistently lower than general software development experience, due to the relative novelty of the technology.

```{r jobAreas, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center', fig.show="hold", fig.cap="Job areas and job titles stated by participants"}

demographics <- demographics %>% rowwise() %>% mutate(jobTitle = if (nchar(jobTitle) == 0) "n/a" else jobTitle)

figureNr <- 4

demographicsPlots = list()
plotIndex <- 1

demographicsPlots[[1]] <- demographics %>% 
  dplyr::select(jobArea) %>% 
  group_by(jobArea) %>% 
  dplyr::summarise(count = n()) %>%
  mutate(ratio = (count / sum(count))) %>%
  arrange(ratio) %>%
  mutate(ratioLabel = scales::percent(ratio)) %>%
ggplot(aes(x = "", y = ratio, fill = jobArea)) +
  geom_col() +
  geom_text(aes(label = ratioLabel),
            position = position_stack(vjust = 0.5),
            size = 3) +
  coord_polar(theta = "y") + 
  #scale_fill_viridis_d(alpha = 0.75, begin = 0.1, end = 0.9, option = "D") +
  scale_fill_brewer(palette="Blues") +
  theme_void() +
  theme(text = element_text(size=10), 
        legend.text = element_text(size=10),
        legend.position = "left",
        #strip.text = element_text(size=10),
        plot.caption = element_text(hjust = 0.5, size = 12,  face= "plain", family = "serif")) +
  labs(caption = "Distribution of reported job areas")

demographicsPlots[[2]] <- demographics %>% 
  dplyr::select(jobTitle) %>% 
  group_by(jobTitle) %>% 
  dplyr::summarise(count = n()) %>%
ggplot(aes(x=reorder(jobTitle, count), y=count)) + 
  geom_bar(stat = "identity", fill="steelblue", alpha = 0.9) +
  scale_y_continuous(breaks=function(limits) {return(sequence(c(limits[2] + 1), from=limits[1], by=1) )}) + 
  theme_ipsum(base_family = "sans") +
  theme(text = element_text(size=10), 
        axis.title.x = element_text(size=10), 
        axis.title.y = element_text(size=10),
        plot.caption = element_text(hjust = 1, size = 12,  face= "plain", family = "serif")) +
  xlab("Job title") +
  ylab("Count") + 
  coord_flip() +
  labs(caption = "Job titles reported by participants")

ggpubr::ggarrange(plotlist=demographicsPlots, ncol=2, nrow=1, widths = c(1,1),  heights = c(1,1), align = "hv")
  

```



```{r experience, echo=FALSE , fig.width=5, fig.height=3, fig.cap="Years of experience reported by participants"}

demographics %>% 
  select(generalExperience, cloudExperience) %>% 
  pivot_longer(everything()) %>%
ggplot(aes(x=fct_rev(name), y=value, fill=name)) + 
  geom_boxplot(fill="steelblue", alpha=0.6, width=0.4) + 
  xlab("Type of experience") + 
  ylab("Years of experience") +
  theme_ipsum(base_family = "sans")

```


\newpage

# Results{#results}

```{r, include=FALSE, echo=FALSE, message = FALSE}
aspectKeys <- setdiff(colnames(surveydata), c("participant", "factorKey", "answerTime"))

addToFrame <- function(data, factorKey) {
  for (aspect in aspectKeys) {
    # extract ratings for this factor x aspect combination
    ratings = pull(data, aspect)
    factorAspectCombination <<- rbind(factorAspectCombination, data.frame(factorKey=factorKey, aspectKey=aspect, highLevelAspectKey=values(aspectInformation, keys=aspect)[3], rating=ratings))
  }
}

factorAspectCombination <- data.frame(factorKey=as.character(), aspectKey=as.character(), highLevelAspectKey=as.character(), rating=as.numeric())
surveydata %>% 
  group_by(factorKey) %>%
  group_walk(addToFrame)

# threshold for the number of answers that should have been given for a factor as a minimum so that the result has some significance
answerThreshold <- 5

# summarize data for each factor-aspect combination and calculate additional metrics
factorAspectCombinationFlat <- factorAspectCombination %>% 
                                group_by(factorKey, aspectKey, highLevelAspectKey) %>% 
                                dplyr::summarise("--"=sum(rating == -2), 
                                                 "-"=sum(rating == -1),
                                                 "0"=sum(rating == 0),
                                                 "+"=sum(rating == 1),
                                                 "++"=sum(rating == 2),
                                                 "nonZeroAnswers"=sum(rating != 0),
                                                 "total"=n()) %>%
                                rowwise() %>% 
                                dplyr::mutate("weightedMean"= sum(`--` * -2, `-` * -1, `+`, `++` * 2) / total, 
                                              "mixedSignals" = (sum(`--`, `-`) > 0 & sum(`+`, `++`) > 0),
                                              "aboveThreshold" = (total >= answerThreshold))

# calculate exact test of goodness-of-fit
assumedDistribution <- c(1,1,2,1,1)

factorAspectCombinationFlat <- factorAspectCombinationFlat %>% rowwise() %>% 
  dplyr::mutate("pValue"= xmulti(c(`--`,`-`,`0`,`+`,`++`),assumedDistribution, detail=1)$pProb )

factorAspectCombinationFlat$pValue <- round(factorAspectCombinationFlat$pValue, digits = 8)

```

## Data characteristics{#characteristics}

The main results of our survey are the aggregated ratings stated by the participants per **factor-quality aspect** combination.
Overall, there is a quite broad distribution of impacts stated by participants. For `r nrow(factorAspectCombinationFlat %>% filter(nonZeroAnswers != 0))` out of the 1080 potential factor-quality aspect combinations there was at least one impact explicitly stated by participants.

To illustrate how the results for these combinations look like, some exemplary combinations with the aggregated data from the survey are shown in the Figures 6-11.
The graphs in these figures show for a single combination, how many times each possible impact rating has been stated by participants. As already mentioned, the impact type ``0`` was set as a default for all ratings. This means that there is a certain probability that a quality aspect has not been considered for a factor by a participant, just because he or she prioritized other quality aspects, but the rating was nevertheless captured as ``0``.

Based on these combinations, we want to derive which impacts exist between the different factors and quality aspects.
As stated in the introduction, we want to use these results to validate the initially stated impacts of the quality model for cloud-native application architectures as well as to include additional impacts.
To prepare the analysis, we calculated the following measures for each factor-quality aspect combination:

* **weightedMean** (``numeric``): The mean value of ratings weighted by the number of times they have been stated. To enable this calculation we assigned values to impact types in the following way: **\-\-** is interpreted as the value ``-2``, **-** as the value ``-1``, **0** as the value ``0``, **+** as the value ``+1``, and **++** as the value ``+2``.
* **aboveThreshold** (``logical``): Is ``TRUE`` if the total number of ratings provided that factor-quality aspect combination is at least the threshold we defined to show some significance. For the analysis we set this threshold to `r answerThreshold`.
* **pValue** (``numeric``): Probability of the observed distribution under the null hypothesis. See the following explanation.

To have an indicator for the significance of a result, we used the [Exact multinomial test of goodness-of-fit](https://www.biostathandbook.com/exactgof.html) which is suitable when there are multiple values of one nominal variable (the different types of impact) and the sample size is small. 
Additionally, the individual observations (answers) need to be independent which we can assume, because participants did not see answers from other participants and did the survey presumably alone.
For this test we need to formulate a theoretically expected distribution which represents the null hypothesis. 
We can then calculate the probability of getting the actually observed distribution under the null hypothesis to assess whether the null hypothesis can be rejected.
As a distribution for the null hypothesis, we assume that the impact of a factor on a quality aspect is undecidable: That means that all possible ratings are equally likely with an exception of the rating **no impact (0)**. 
We consider the rating **0** as being twice as likely as the other ratings, because it is selected as the default if the participant did not explicitly state a different rating.
For the ratings **\-\-**:**-**:**0**:**+**:**++**, we thus assume a ratio of 1:1:2:1:1 under the null hypothesis.
For each factor-quality aspect combination, we can therefore calculate the p value using this test, which we added as **pValue** to each combination.
Again, it has to be noted, that these values depend on the number of participants who decided to provide an answer for the respective factor and are therefore not representative for the sample as a whole.

In [section 5.3](#validation), we describe how we validated the initially stated impacts from the quality model using these measures. And in [section 5.4](#additionalImpacts), we explore additionally stated impacts to integrate them in the quality model.

\newpage

## Examples{#resultExamples}

Exemplary factor-quality aspect combinations with the results from the survey to illustrate the data.

```{r, echo=FALSE, results='hide', message=FALSE, fig.show="hold", fig.width=8, fig.height=10, fig.align='center', fig.pos="h"}

exampleCombinations <- list(c("automatedRestarts", "recoverability"),
                     c("built-InAutoscaling", "elasticity"),
                     c("consistentCentralizedMetrics", "analyzability"),
                     c("retriesForSafeInvocations", "faultTolerance"),
                     c("serviceReplication", "timeBehaviour"),
                     c("mediatedCommunication", "timeBehaviour")
                     )

plots = list()
plotIndex <- 1

plotFactorAspectCombination <- function(paramFactorKey, paramAspectKey) {
  
  combination <- factorAspectCombination %>% filter(factorKey==paramFactorKey, aspectKey==paramAspectKey)
  
  plot <- ggplot(combination, aes(x=rating)) + 
    geom_bar(stat = "count", fill="steelblue", alpha = 0.9, width=0.5) + 
    scale_x_continuous(breaks=c(-2,-1,0,1,2), labels=c("--", "-", "0", "+", "++"), limits = c(-2.5,2.5), minor_breaks=NULL) + 
    scale_y_continuous(limits = function(limits) {return(c(limits[1], ceiling(limits[2])))}, breaks = function(limits) {
      calculatedBreaks <- floor(seq(from=0, to=limits[2], length.out=5))
      return(calculatedBreaks)
      }) + 
    
    theme_ipsum(base_family = "sans") +
    theme(text = element_text(size=12), 
          panel.grid.minor = element_blank(),
          axis.text.x = element_text(size=12, vjust=0.2), 
          axis.text.y = element_text(size=12, vjust=0.4), 
          axis.title.x = element_text(size=11),
          axis.title.y = element_text(size=11),
          axis.line = element_line(colour = "black"),
          axis.ticks.x = element_line(colour = "black", linewidth = 0.5, linetype = "solid"),
          axis.ticks.y = element_line(colour = "black", linewidth = 0.5, linetype = "solid"),
          plot.title = element_textbox_simple(size = 13,  face= "bold", margin = ggplot2::margin(0, 0, 0, 3)),
          plot.caption = element_text(hjust = 0.5, size = 12,  face= "plain", family = "serif")
          ) +
          
    xlab("Impact type") + # xlab("Impact strength: -2: strongly negative, -1: negative, 0: no impact, +1: positive, +2: strongly positive")
    ylab("Times rated") +
    ggtitle(paste(values(factorInformation, keys=paramFactorKey)[1], "‚Üí", values(aspectInformation, keys=paramAspectKey)[1])) +
    labs(caption = paste("Figure", paste(figureNr, ":", sep = ""), " Ratings for the impact from\n", values(factorInformation, keys=paramFactorKey)[1], (ifelse(nchar(paramFactorKey)>30, "\non", "on")), values(aspectInformation, keys=paramAspectKey)))
  
    #print(plot) # to directly print a plot
    plots[[plotIndex]] <<- plot
    plotIndex <<- plotIndex + 1
    figureNr <<- figureNr + 1
}

figureNr <- 6

for (example in exampleCombinations) {
  plotFactorAspectCombination(example[1], example[2])
}

ggpubr::ggarrange(plotlist=plots, ncol=2, nrow=3, widths = c(1,1),  heights = c(1,1), align = "hv")


```

\newpage

## Validation of previously stated impacts{#validation}

To assess whether the impacts stated in our initial quality model are confirmed and therefore validated by the survey, we mapped the results for each factor-quality aspect combination to the factor-quality aspect combination with a hypothesized impact from the initial quality model. 
For each combination, we then used the following R function to decide on whether the hypothesized impact can be validated or not:

```{r validationFunction, echo=TRUE}

validateImpact <- function(statedImpact, weightedMean, 
                           aboveThreshold, pValue) {
  
  pThreshold <- 0.1
  validationResult <- ""
  
  if(weightedMean < 0.5 & weightedMean > -0.5) {
    validationResult <- if(aboveThreshold & pValue < pThreshold) "‚úó" else "(‚úó)"
  } else {
    impactTrend <- if(weightedMean > 0) "positive" else "negative"
    if (aboveThreshold & pValue < pThreshold) {
      validationResult <- if(impactTrend == statedImpact) "‚úì" else "‚áÑ"
    } else {
      validationResult <- if(impactTrend == statedImpact) "(‚úì)" else "(‚áÑ)"
    }
  }
  return(validationResult)
}

```

The function receives the following input parameters:

* **statedImpact** (``character``): The impact stated in the initial quality model. Is either ``positive`` or ``negative``
* **weightedMean**, **aboveThreshold**, and **pValue** as described in [section 5.1](#characteristics)

and returns a ``character`` value describing the validation result. 

In essence, an impact is considered valid if the **weightedMean** value is beyond a limit of |0.5| and has the same leading sign as expected based on the hypothesized impact. 
Additionally, we used a significance level of …ë = 0.1. 
If the p Value is not below …ë or the number of answers for that factor is below the set threshold, results are marked as only probable, using brackets.
The p Value additionally works as an indicator for how probable a result is.
Finally, it is also possible that the analysis of the results shows an impact inverse to the hypothesized one.
The possible outcomes are also listed in Table 1.

| Result | Explanation                                           |
|:------:|-------------------------------------------------------|
| ‚úì      | The impact is valid.                                  |
| (‚úì)    | The impact is potentially valid.                      |
| ‚úó      | The impact is invalid.                                |
| (‚úó)    | The impact is potentially invalid.                    |
| ‚áÑ      | The impact is inverse.                                |
| (‚áÑ)    | The impact is potentially inverse.                    |

Table: Possible results for the validation of impacts.

```{r , echo = FALSE, message=FALSE}

deriveImpact <- function(weightedMean) {
  if (is.na(weightedMean)) {
    return("?")
  } else if (weightedMean >= 1.5) {
    return ("++")
  } else if (weightedMean > 0) {
    return ("+")
  } else if (weightedMean == 0) {
    return ("0")
  } else if (weightedMean < 0 & weightedMean > -1.5) {
    return ("-")
  } else if (weightedMean <= -1.5) {
    return ("--")
  }
}

impactValidation <- left_join(initialImpacts, factorAspectCombinationFlat, c("factorKey" = "factorKey", "qualityAspectKey" = "aspectKey"))
impactValidation <- impactValidation %>% rowwise() %>% mutate(derivedImpact = deriveImpact(weightedMean)) %>% mutate(validation = validateImpact(impact, weightedMean, aboveThreshold, pValue))

impactValidation$factorName <- sapply(impactValidation$factorKey, function(key) values(factorInformation, keys=key)[1])
impactValidation$aspectName <- sapply(impactValidation$qualityAspectKey, function(key) values(aspectInformation, keys=key)[1])
impactValidation$highLevelAspectName <- sapply(impactValidation$qualityAspectKey, function(key) values(aspectInformation, keys=key)[4])

columnSelection <- c("factorName", "aspectName", "impact", "--", "-", "0", "+", "++", "derivedImpact", "validation", "pValue")
columnNames <- c("Factor", "Quality Aspect", "hypothesized", "- -", "-", "0", "+", "++", "Impact", "Validation", "p-Value")
columnAlignment <- "lllccccccc"
withTableStyling<- function(kTable) {
  kTable %>%
  row_spec(0,bold=TRUE) %>% 
  column_spec(1, width = "15em", latex_valign = "m") %>%
  column_spec(c(2:3), latex_valign = "m") %>%
  column_spec(c(4:11), latex_valign = "m") %>%
  kable_styling(font_size = 10, latex_options=c("scale_down", "HOLD_position")) %>%
  collapse_rows(columns=1)
}

if (knitr::is_html_output()) {
  # escape markdown characters for html
  columnNames <- c("Factor", "Quality Aspect", "hypothesized", "\\-\\-", "\\-", "0", "\\+", "++", "Impact", "Validation", "p-Value")
  impactValidation$derivedImpact <- sapply(impactValidation$derivedImpact, function(unescaped) {return(str_replace_all(unescaped, "[+-]", "\\\\\\0"))} )
}

```

In the following, we have grouped the outcomes by the individual factors for a better overview and have further separated the factors based on the validation results.

Table \@ref(tab:factorsWithValidImpacts) includes all factors for which at least one hypothesized impact could be confirmed. 
Table \@ref(tab:factorsWithProbablyValidImpacts) lists remaining factors with at least potentially valid impacts and finally Table \@ref(tab:factorsWithInvalidImpacts) lists those factors for which no hypothesized impacts could be validated or where the result is unclear.
Especially the results which are controversial, such as the impact from *Resource limits* on *Resource utilization* in Table \@ref(tab:factorsWithInvalidImpacts), indicate that these combinations need further investigation.
Possibly, the descriptions provided for these factors and quality aspects can be interpreted in different ways or the factors cover different characteristics which impact quality aspects differently. 
In any case these factors need to be revised either by describing more clearly what characteristics they cover or by exchanging them with separate factors that cover individual characteristics which have differing impacts on quality aspects.

Overall, several impacts stated in the initial quality model can be validated with the survey and for many there is at least a tendency towards their validation. 
By tendency we mean that for these impacts individual ratings supported the initially stated impacts, but there were simply not enough answers to rate the impact as validated based on our evaluation. 

However, many impacts also could not be validated and might need to be removed from the quality model. 
One reason why impacts that were stated in the initial quality model could now not be validated might be that we did not consider mediating factors in the survey. 
Mediating factors are those factors in a hierarchical quality model which impact quality aspects, but are also impacted by other factors. 
In a sense they "explain" the relationship between lower-level factors and higher-level quality aspects. 
But when they are missing, an indirect impact from a lower-level factor on a quality aspect might be less obvious. 
Furthermore, especially the factors in Table \@ref(tab:factorsWithInvalidImpacts) consider more abstract characteristics which might be less intuitive to understand and therefore more difficult to rate.
Nevertheless, it can make the quality model simpler and therefore better understandable if impacts that could not be validated are removed.



```{r factorsWithValidImpacts, echo = FALSE, message=FALSE}

# filtered by valid impacts
validatedImpacts <- impactValidation %>% filter(validation == "‚úì")

factorKeysValidated <- unique(validatedImpacts$factorKey)

withTableStyling(kable((impactValidation %>% filter(factorKey %in% factorKeysValidated) %>% arrange(factorKey))[,columnSelection], 
      caption = "Factors with a successful validation of initially stated impacts",
      col.names = columnNames,
      longtable = FALSE,
      align = columnAlignment
      ))
```


```{r factorsWithProbablyValidImpacts, echo = FALSE, message=FALSE}

# filter by potentially valid impacts
potentiallyValidatedImpacts <- impactValidation %>% filter(validation == "(‚úì)")

factorKeysPotentiallyValidated <- setdiff(unique(potentiallyValidatedImpacts$factorKey),  factorKeysValidated)

withTableStyling(kable((impactValidation %>% filter(factorKey %in% factorKeysPotentiallyValidated) %>% arrange(factorKey))[,columnSelection], 
      caption = "Factors with potentially valid initially stated impacts",
      col.names = columnNames,
      longtable = FALSE,
      align = columnAlignment
      ))
```


```{r factorsWithInvalidImpacts, echo = FALSE, message=FALSE}

# remaining factors with invalid or unclear results
factorKeysRemaining <- setdiff(unique(impactValidation$factorKey), union(factorKeysValidated, factorKeysPotentiallyValidated))

withTableStyling(kable((impactValidation %>% filter(factorKey %in% factorKeysRemaining) %>% arrange(factorKey))[,columnSelection], 
      caption = "Factors with (potentially) invalid and unclear initially stated impacts",
      col.names = columnNames,
      longtable = FALSE,
      align = columnAlignment
      ))
```

\newpage

## Additionally found impacts{#additionalImpacts}

Next, we analyzed the survey results for impacts that were not previously considered in the quality model. 
As written before, there is a broad range of factor-quality aspect combinations for which ratings were stated by the participants. 
The challenge therefore is to filter out the significant ones which can potentially be included in the quality model. 

To determine the type and strength of each impact, we again used the metrics described in [section 5.1](#characteristics).
We categorized the different impacts according to their type and probability by comparing the **weightedMean** value with the same limits as in the previous validation analysis and the p-Value with the same significance level as in the previous analysis.

```{r newImpactFunction, echo=FALSE}

deriveNewImpact <- function(weightedMean, aboveThreshold, pValue) {
  
  pThreshold <- 0.1
  derivedImpact <- "" # can in the end be one of -- | - | 0 | + | ++ with surrounding brackets () if the answer is uncertain
  
  if (weightedMean == 0) {
      derivedImpact <- if (aboveThreshold & pValue < pThreshold) "0" else "(0)"
  } else {
    impactTrend <- if(weightedMean > 0) "positive" else "negative"
    if (aboveThreshold & pValue < pThreshold) {
      if (abs(weightedMean) >= 1.5) {
        derivedImpact <- if (impactTrend == "positive") "++" else "--"
      } else if (abs(weightedMean) >= 0.5) {
        derivedImpact <- if (impactTrend == "positive") "+" else "-"
      } else { # abs(weightedMean) is between 0 and 0.5
        derivedImpact <- "0"
      }
    } else {
      if (abs(weightedMean) >= 1.5) {
        derivedImpact <- if (impactTrend == "positive") "(++)" else "(--)"
      } else if (abs(weightedMean) >= 0.5) {
        derivedImpact <- if (impactTrend == "positive") "(+)" else "(-)"
      } else { # abs(weightedMean) is smaller than 0.5
        derivedImpact <- "(0)"
      }
    }
  }
  return(derivedImpact)
}

```

The resulting factor-quality aspect combinations can be seen in the Tables \@ref(tab:newImpactsToConsider) and \@ref(tab:newImpactsToConsider2).
However we only included impacts which were not already considered in the quality model (and therefore the validation step) and from those only impacts showing a positive or negative trend.

The impacts are grouped by the factors as well as the high-level quality aspects of the quality aspects impacted by factors. 
Therefore, it can be seen that for several factors, impacts on quality aspects of the same high-level quality aspect have been rated. 
This shows a potential difficulty of using the quality aspects from the ISO 25000 standard: Because the quality aspects which we used for the survey are strongly related based on their common high-level quality aspect, it is more difficult to clearly distinguish between them in terms of how they are affected by software characteristics.
For example, the factor *Managed infrastructure* in Table \@ref(tab:newImpactsToConsider2) has been rated as positively impacting all quality aspects grouped under the *Reliability* high-level aspect. 
In a hierarchical quality model, however, it would be ideal if there is only one path from a factor to a single top-level quality aspect. For the quality model this case could thus mean that either the quality aspects as considered in the survey should be used as the top-level aspects, excluding the high-level aspects or that the factor *Managed infrastructure* might need to be separated into separate other factors which then exclusively impact the different quality aspects. 
The survey results therefore show aspects which need to reconsidered in the quality model and can guide the future work on it to develop a conceptually sound quality model.

```{r additionalImpacts, echo = FALSE, message=FALSE}

newImpacts <- anti_join(factorAspectCombinationFlat, initialImpacts, c("factorKey" = "factorKey", "aspectKey" = "qualityAspectKey"))
newImpacts <- newImpacts %>% mutate(derivedImpact = deriveNewImpact(weightedMean, aboveThreshold, pValue))

newImpactsFiltered <- newImpacts %>% filter(derivedImpact != "0" & derivedImpact != "(0)" & aboveThreshold)

newImpactsFiltered <- newImpactsFiltered %>% 
                      group_by(factorKey, highLevelAspectKey)

newImpactsFiltered$factorName <- sapply(newImpactsFiltered$factorKey, function(key) values(factorInformation, keys=key)[1])
newImpactsFiltered$aspectName <- sapply(newImpactsFiltered$aspectKey, function(key) values(aspectInformation, keys=key)[1])
newImpactsFiltered$highLevelAspectName <- sapply(newImpactsFiltered$aspectKey, function(key) values(aspectInformation, keys=key)[4])

columnSelection <- c("factorName", "aspectName", "highLevelAspectName", "--", "-", "0", "+", "++", "derivedImpact", "pValue")
columnNames <- c("Factor", "Quality Aspect", "High-level Aspect", "- -", "-", "0", "+", "++", "Impact", "p-Value")
columnAlignment <- "llccccccc"

withTableStyling <- function(kTable) {
  kTable %>%
  row_spec(0,bold=TRUE) %>% 
  column_spec(1, width = "25em", latex_valign = "m") %>%
  column_spec(2, latex_valign = "m") %>%
  column_spec(c(3:9), latex_valign = "m") %>%
  kable_styling(font_size =10, latex_options=c("scale_down", "HOLD_position")) %>%
  collapse_rows(columns=c(1,3), target=1)
}

if (knitr::is_html_output()) {
  # escape markdown characters for html
  columnNames <- c("Factor", "Quality Aspect", "High-level Aspect", "\\-\\-", "\\-", "0", "\\+", "++", "Impact", "p-Value")
  newImpactsFiltered$derivedImpact <- sapply(newImpactsFiltered$derivedImpact, function(unescaped) {return(str_replace_all(unescaped, "[+-]", "\\\\\\0"))} )
}


```

```{r newImpactsToConsider, echo = FALSE, message=FALSE}

newImpactsToOutput <- newImpactsFiltered %>% arrange(highLevelAspectKey) %>% arrange(factorKey)

withTableStyling(kable((newImpactsToOutput[1:29,])[, columnSelection],
      caption = "Factors with new impacts (potentially) considerable for the quality model",
      col.names = columnNames,
      longtable = FALSE,
      align = columnAlignment
      ))

```

```{r newImpactsToConsider2, echo = FALSE, message=FALSE}

withTableStyling(kable((newImpactsToOutput[30:nrow(newImpactsToOutput),])[, columnSelection],
      caption = "Factors with new impacts (potentially) considerable for the quality model continued",
      col.names = columnNames,
      longtable = FALSE,
      align = columnAlignment
      ))

```


# Conclusion{#conclusion}

The results of this survey will be used to continue the work on the quality model for cloud-native application software architectures. 
While parts of the quality model could be validated by the results of the survey, others need reconsideration. 
Our plan for future work is to apply the quality model to software architectures functioning as use cases based on which we want to further validate the elements of the quality model. 
The insights from this survey can guide this work by showing which factors to focus on or for example which hypotheses to formulate for experiments with such software architectures.

Although we received a lower number of submissions than we had initially hoped for, the general approach for the survey showed feasible. 
But the lower number of submission also means that the interpretation of the results needs to be done with caution which we did by considering the significance of results and marking results as uncertain if applicable. 
